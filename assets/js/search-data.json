{
  
    
        "post0": {
            "title": "Sanskrit Albert",
            "content": ". !nvidia-smi . Sat May 2 06:43:02 2020 +--+ | NVIDIA-SMI 440.64.00 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P4 Off | 00000000:00:04.0 Off | 0 | | N/A 31C P8 7W / 75W | 0MiB / 7611MiB | 0% Default | +-+-+-+ +--+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +--+ . import os import gc import glob import torch import pickle import joblib from tqdm.auto import tqdm . HuggingFace Recently updated their scripts, and the pip is yet to be released. So We&#39;ll build from source . !pip install tokenizers #!pip install transformers . !git clone https://github.com/huggingface/transformers !pip install transformers/. . Collecting Corpus . I have used Sanskrit Corpus from Kaggle dataset. Feel free to skip and use your own ddataset. The trainig data needs to be in a .txt file. and I have also used Evaluation using the same dataset. . I need Kaggle API to download the dataset. You can load your text corpus from anywhere. . You can download corpus for your language from https://traces1.inria.fr/oscar. . I have used data fro mthere too, and appended the data to corpus from Kaggle. . Loading From kaggle . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . Kaggle dataset link . Thanks to inltk fro wikipedia dumps and CLTK from which I am currently collecting Sanskrit scraps from open sources. . !mkdir corpus #directory for sac=ving all corpus in a single directory. You can save it anywhere #From Kagle !kaggle datasets download -d disisbig/sanskrit-wikipedia-articles !unzip /content/sanskrit-wikipedia-articles.zip -d /content/corpus #From OSCAR corpus !wget https://traces1.inria.fr/oscar/files/compressed-orig/sa.txt.gz !gunzip /content/sa.txt.gz . #Reading sample with open(&quot;/content/sa.txt&quot;, &quot;r&quot;) as fp: print(fp.read(1000)) . import glob train_list = glob.glob(&quot;/content/corpus/train/train/*.txt&quot;) valid_list = glob.glob(&quot;/content/corpus/valid/valid/*.txt&quot;) . #readig and appending all small files to single Train and Valid files with open(&quot;/content/corpus/train/full.txt&quot;, &quot;wb&quot;) as outfile: for f in train_list: with open(f, &quot;rb&quot;) as infile: outfile.write(infile.read()) outfile.write(b&quot; n n&quot;) with open(&quot;/content/sa.txt&quot;, &quot;rb&quot;) as infile: outfile.write(infile.read()) with open(&quot;/content/corpus/valid/full_val.txt&quot;, &quot;wb&quot;) as outfile: for f in valid_list: with open(f, &quot;rb&quot;) as infile: outfile.write(infile.read()) outfile.write(b&quot; n n&quot;) . Tokenizer Training . Directory to save trained tokenier and configuration files in a folder . !mkdir data_dir . import sentencepiece as spm from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer . %%time #Albert Tokenizer uses Sentence piece Tokenization, so I have used sentencepiece to to train tokenizer. #This will take a while spm.SentencePieceTrainer.Train(&#39;--input=/content/corpus/train/full.txt --model_prefix=m --vocab_size=32000 --control_symbols=[CLS],[SEP],[MASK]&#39;) with open(&quot;m.vocab&quot;) as v: print(v.read(2000)) v.close() . !mkdir /content/data_dir/ !cp /content/m.model -d /content/data_dir/spiece.model !cp /content/m.vocab -d /content/data_dir/spiece.vocab . mkdir: cannot create directory ‚Äò/content/data_dir/‚Äô: File exists . Testing Tokenizer . Make sure to checkout the Fast Tokenizers from Huggingface, Tis is really Fast! You can compare with sentencepiece. . %time tokenizer = SentencePieceBPETokenizer() tokenizer.train(&quot;/content/corpus/train/full.txt&quot;) . CPU times: user 4 ¬µs, sys: 0 ns, total: 4 ¬µs Wall time: 10 ¬µs . This is a very beautiful Shlok ‚ù§Ô∏è, Let&#39;s just pray for this üôè. Do search online if you are interested! . txt = &quot;‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á‡§§‡•ç‡§∞ ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É ‡§∏‡§®‡•ç‡§§‡•Å| ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É| ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å| ‡§Æ‡§æ‡§Å ‡§ï‡§∂‡•ç‡§ö‡§ø‡§¶‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§Æ‡§æ‡§™‡•ç‡§®‡•Å‡§Ø‡§æ‡§§‡•• ‡•ê ‡§∂‡§æ‡§Ç‡§§‡§ø‡§É ‡§∂‡§æ‡§Ç‡§§‡§ø‡§É ‡§∂‡§æ‡§Ç‡§§‡§ø‡§É ‡••&quot; enc = tokenizer.encode(txt) print(tokenizer.decode(enc.ids)) . ‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á‡§§‡•ç‡§∞ ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É ‡§∏‡§®‡•ç‡§§‡•Å| ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É| ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å| ‡§Æ‡§æ‡§Å ‡§ï‡§∂‡•ç‡§ö‡§ø‡§¶‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§Æ‡§æ‡§™‡•ç‡§®‡•Å‡§Ø‡§æ‡§§‡•• ‡•ê ‡§∂‡§æ‡§Ç‡§§‡§ø‡§É ‡§∂‡§æ‡§Ç‡§§‡§ø‡§É ‡§∂‡§æ‡§Ç‡§§‡§ø‡§É ‡•• . The tokenizer ssems to work, But since, The training script is configured to use Albert tokenizer. we need to use spiece.model and spiece.vocab, for training script . HuggingFace tokenizer creates [&#39;/content/hft/vocab.json&#39;, &#39;/content/hft/merges.txt&#39;] . files, while the AlbertTokenizer requires spiece.model file. So we&#39;ll use sentencepiece saved vocab and tokenizer model . !mkdir hft tokenizer.save(&quot;/content/hft&quot;) #we won&#39;t be using this . [&#39;/content/hft/vocab.json&#39;, &#39;/content/hft/merges.txt&#39;] . Huggingface Training . from transformers import * . #Keep in mind, This is a tokenizer for Albert, unlike the previous one, which is a generic one. #We&#39;ll load it in the form of Albert Tokenizer. tokenizer = AlbertTokenizer.from_pretrained(&quot;/content/data_dir&quot;) . op = tokenizer.encode(&quot;‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å‡•§ ‡§Æ‡§æ ‡§ï‡§∂‡•ç‡§ö‡§ø‡§§‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§≠‡§æ‡§ó‡•ç‡§≠‡§µ‡•á‡§§‡•ç‡•• ‡•ê ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡••&quot;) tokenizer.decode(op) . &#39;[CLS] ‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å‡•§ ‡§Æ‡§æ ‡§ï‡§∂‡•ç‡§ö‡§ø‡§§‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§≠‡§æ‡§ó‡•ç‡§≠‡§µ‡•á‡§§‡•ç‡•• ‡•ê ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡••[SEP]&#39; . Model-Tokenizer Configurtion . This is important. The training script needs a configuration for the model. . Architecture refers to what the model is going to be used for i.e., AlbertModelForLM, or for Sequence Classification. Just take a look ar left panel for Model Architectures . #Checking vocabulary size vocab_size=tokenizer.vocab_size ; vocab_size . 32000 . import json config = { &quot;architectures&quot;: [ &quot;AlbertModel&quot; ], &quot;attention_probs_dropout_prob&quot;: 0.1, &quot;hidden_act&quot;: &quot;gelu&quot;, &quot;hidden_dropout_prob&quot;: 0.1, &quot;hidden_size&quot;: 768, &quot;initializer_range&quot;: 0.02, &quot;intermediate_size&quot;: 3072, &quot;layer_norm_eps&quot;: 1e-05, &quot;max_position_embeddings&quot;: 514, &quot;model_type&quot;: &quot;albert&quot;, &quot;num_attention_heads&quot;: 12, &quot;num_hidden_layers&quot;: 6, &quot;type_vocab_size&quot;: 1, &quot;vocab_size&quot;: vocab_size } with open(&quot;/content/data_dir/config.json&quot;, &#39;w&#39;) as fp: json.dump(config, fp) #Configuration for tokenizer. #Note I havve set do_lower_case: False, and keep_accents:True tokenizer_config = { &quot;max_len&quot;: 512, &quot;model_type&quot;: &quot;albert&quot;, &quot;do_lower_case&quot;:False, &quot;keep_accents&quot;:True } with open(&quot;/content/data_dir/tokenizer_config.json&quot;, &#39;w&#39;) as fp: json.dump(tokenizer_config, fp) . Note: While experimenting with tokenizer training, I found that encoding was done corectly, but when decoding with {do_lower_case: True, and keep_accents:False}, the decoded sentence was a bit changed. . So, by using above settings, I got the sentences decoded perfectly. a reason maybe that Sanskrit does not have &#39;Casing&#39;. and the word has suffixes in the form of accents. . You should try with the settings ehich suits best for your langugae. . torch.cuda.empty_cache() gc.collect() . 157 . Creating a small corpus for testing, You can skip this. . with open(&quot;/content/corpus/train/tmp.txt&quot;, &quot;w&quot;) as fp: fp.write(open(&quot;/content/corpus/train/full.txt&quot;, &quot;r&quot;).read(100000)) #250KB . with open(&quot;/content/corpus/valid/val_val.txt&quot;, &quot;w&quot;) as fp: fp.write(open(&quot;/content/corpus/valid/full_val.txt&quot;, &quot;r&quot;).read(10000000)) # . Checkpointing is very important. This is a directory where the intermediate model and tokenizer will be saved. . Note: You should checkpoint to somewhere else, Maybe to your drive. and set --save_total_limit 2 . This is the training script. you should experiment with arguments. . !python /content/transformers/examples/run_language_modeling.py --help . %load_ext tensorboard %tensorboard --logdir logs . You see the maguc here. . This script can be used to triain most models with for Language modelling. . Another thing, Observe that you have to directly specify --training_data_file in .txt format. No need to generate any pretraining data! all thanks to the Fast toknizers in used for loading the text. . Features are created dynamically while starting trainng script. However, This is limited to GPUs only. I would love to see a TPU version too. . Make sure to change batch_sizes according to the GPU you are having. I set to 16 because of 8 GB P4, . #To train from scratch !python /content/transformers/examples/run_language_modeling.py --model_type albert-base-v2 --config_name /content/data_dir/ --tokenizer_name /content/data_dir/ --train_data_file /content/corpus/train/full.txt --eval_data_file /content/corpus/valid/full_val.txt --output_dir /content/data_dir --do_train --do_eval --mlm --line_by_line --save_steps 500 --logging_steps 500 --save_total_limit 2 --evaluate_during_training --num_train_epochs 5 --per_gpu_eval_batch_size 16 --per_gpu_train_batch_size 16 --block_size 256 --seed 108 --should_continue --logging_dir logs . torch.cuda.empty_cache() gc.collect() . Continuing Training . --model_name_or_path #Refers to the checkpoint directory --overwrite_output_dir #This is used to continue fro mlast checkpoint . After a checkpoint, You just need that directory and the corpus files, and toknizer. All configs, models, oprimizers are saved in --output_dir except tokenizer. . #To continue from checkpoint #I have continued from 500 steps here, but you should use the latet saved models !python /content/transformers/examples/run_language_modeling.py --model_name_or_path /content/data_dir/checkpoint-500 --model_type albert-base-v2 --config_name /content/data_dir/ --tokenizer_name /content/data_dir/ --train_data_file /content/corpus/train/full.txt --eval_data_file /content/corpus/valid/full_val.txt --output_dir /content/data_dir --do_train --do_eval --mlm --line_by_line --save_steps 500 --logging_steps 500 --save_total_limit 2 --num_train_epochs 5 --evaluate_during_training --per_gpu_eval_batch_size 64 --per_gpu_train_batch_size 64 --block_size 256 --seed 108 --should_continue --overwrite_output_dir . Saving for Uploading . Since, training is complete, We can now upload models to Huffingface&#39;s Models . !mkdir sanskrit_albert . atokenizer = AlbertTokenizer.from_pretrained(&quot;/content/data_dir&quot;) atokenizer.save_pretrained(&quot;/content/sanskrit_albert&quot;) . op = atokenizer.encode(&quot;‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å‡•§ ‡§Æ‡§æ ‡§ï‡§∂‡•ç‡§ö‡§ø‡§§‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§≠‡§æ‡§ó‡•ç‡§≠‡§µ‡•á‡§§‡•ç‡•• ‡•ê ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡••&quot;) print(atokenizer.decode(op)) . [CLS] ‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å‡•§ ‡§Æ‡§æ ‡§ï‡§∂‡•ç‡§ö‡§ø‡§§‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§≠‡§æ‡§ó‡•ç‡§≠‡§µ‡•á‡§§‡•ç‡•• ‡•ê ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡••[SEP] . #I am using chackoint because os not much training model = AlbertModel.from_pretrained(&quot;/content/data_dir/checkpoint-500&quot;) model.save_pretrained(&quot;/content/sanskrit_albert&quot;) . Now All the files we want are in a separate folder, Which is all we need to upoad. . Tests . tokenizer = AlbertTokenizer.from_pretrained(&quot;/content/sanskrit_albert&quot;) . txt = &quot;‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å‡•§ ‡§Æ‡§æ ‡§ï‡§∂‡•ç‡§ö‡§ø‡§§‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§≠‡§æ‡§ó‡•ç‡§≠‡§µ‡•á‡§§‡•ç‡•• ‡•ê ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡••&quot; op = tokenizer.encode(txt) . tokenizer.decode(op) . &#39;[CLS] ‡•ê ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§ ‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§¶‡•ç‡§∞‡§æ‡§£‡§ø ‡§™‡§∂‡•ç‡§Ø‡§®‡•ç‡§§‡•Å‡•§ ‡§Æ‡§æ ‡§ï‡§∂‡•ç‡§ö‡§ø‡§§‡•ç ‡§¶‡•Å‡§É‡§ñ ‡§≠‡§æ‡§ó‡•ç‡§≠‡§µ‡•á‡§§‡•ç‡•• ‡•ê ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡§∂‡§æ‡§®‡•ç‡§§‡§ø‡§É ‡••[SEP]&#39; . ps = model(torch.tensor(op).unsqueeze(1)) . print(ps[0].shape) . torch.Size([30, 1, 768]) . This way you can get the embeddings for a sentence . Uploading to Models . Instructions to upload a model . !transformers-cli login . Make sure your model name is the folder name in which this will be uploaded. . Thus, my model would be surajp/sanskrit_albert, but I won&#39;t upload this as I have alreasy uploaded one. . !transformers-cli upload /content/sanskrit_albert . And It&#39;s done! Since, I have alreadu uploaded a model, You can load using surajp/sanskrit-base-albert . #this way tokenizer = AutoTokenizer.from_pretrained(&quot;surajp/albert-base-sanskrit&quot;) model = AutoModel.from_pretrained(&quot;surajp/albert-base-sanskrit&quot;) . enc=tokenizer.encode(&quot;‡§Ö‡§™‡§ø ‡§∏‡•ç‡§µ‡§∞‡•ç‡§£‡§Æ‡§Ø‡•Ä ‡§≤‡§ô‡•ç‡§ï‡§æ ‡§® ‡§Æ‡•á ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡§£ ‡§∞‡•ã‡§ö‡§§‡•á ‡•§ ‡§ú‡§®‡§®‡•Ä ‡§ú‡§®‡•ç‡§Æ‡§≠‡•Ç‡§Æ‡§ø‡§∂‡•ç‡§ö ‡§∏‡•ç‡§µ‡§∞‡•ç‡§ó‡§æ‡§¶‡§™‡§ø ‡§ó‡§∞‡•Ä‡§Ø‡§∏‡•Ä ‡••&quot;) print(tokenizer.decode(enc)) . [CLS] ‡§Ö‡§™‡§ø ‡§∏‡•ç‡§µ‡§∞‡•ç‡§£‡§Æ‡§Ø‡•Ä ‡§≤‡§ô‡•ç‡§ï‡§æ ‡§® ‡§Æ‡•á ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡§£ ‡§∞‡•ã‡§ö‡§§‡•á ‡•§ ‡§ú‡§®‡§®‡•Ä ‡§ú‡§®‡•ç‡§Æ‡§≠‡•Ç‡§Æ‡§ø‡§∂‡•ç‡§ö ‡§∏‡•ç‡§µ‡§∞‡•ç‡§ó‡§æ‡§¶‡§™‡§ø ‡§ó‡§∞‡•Ä‡§Ø‡§∏‡•Ä ‡••[SEP] . ps = model(torch.tensor(enc).unsqueeze(1)) . ps[0].shape . torch.Size([19, 1, 768]) . I hope This notebook was helpful.ü§ó . #StaySafe . This training contained only a little portion of Sanskrit literature. There is a vast amount of literature there which I am collecting. This was only a checkpoint for trainng, I will train more once I collect more data. . I am also trainig for other Indian Languages on different models (Gujarati, Hindi for now). . If you know any resources, Please write to me. I&#39;d love to have your contribution. . parmarsuraj99@gmail.com .",
            "url": "https://parmarsuraj99.github.io/suraj-parmar/jupyter/nlp/huggingface/2020/05/02/SanskritALBERT.html",
            "relUrl": "/jupyter/nlp/huggingface/2020/05/02/SanskritALBERT.html",
            "date": " ‚Ä¢ May 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "ELECTRA",
            "content": "If you are interested in Deep Learning (especially NLP), then you might have heard of BERT and its friends(bigger and smaller ones!). At the heart of these models is a very &quot;Attention&quot; seeking architecture called &quot;Transformer&quot;. . ELECTRA released by Google in March 2020, uses a novel approach for training on corpora. Unlike BERT and co. . To understand this, we must first explore two ideas: . How language models(using Transformers) learn | An overview of Generative models(GAN training) | . but before diving into these, Let&#39;s have a rough idea of Deep Learning in NLP. . Representing Words in numbers . A word must be represented as a vector before feeding it to a deep learning model. This can be done using so many ways (one-hot encoding, Word2Vec, pretrained embeddings). Think of this as a vocabulary of N words, and every word represented using a d dimensional vector. . Language Modelling . We can think deep learning models as Universal Function approximators, So how do they learn the langugage? how they represent? Turns out, we can train them by asking them to predict the next word in a sentence by using the context of previous words. . Let&#39;s do a small exercise. Try to predict the next word. . During this pandemic of COVID-19, Everyone should wear a ... . | Because of the dress code, Everyone should wear a ... . | For the first, the probability of the word &#39;mask&#39; is very high. (Although, It should be complete 1. Stay Safe!) . and for the second one, maybe a type of clothing. . So, this is called Language Modelling(LM), in which we try to learn the language(structure, semantics and hopefully everything) mathematically by predicting next word using previous words(context). . Masked Language Modelling . Is predicting next word the only way to learn? Not really, BERT used something called MLM(Masked Language Modelling). . Unlike sequential models(RNN, LSTM, GRU) which takes one word at a timestep, Transformer Encoder of BERT can take entire sequence of input together and process it. so there&#39;s no meaning of predicting next word, as the next word would be exposed to internally. This led to masked tokens. Randomly, replacing 15% of tokens with &quot;[MASK]&quot; token and asking model to predict the most probable word for &quot;[MASK]&quot; . During this pandemic of COVID-19, Everyone should [MASK] a mask. | Because of the dress code, Everyone should [MASK] traditional clothes at party. | Now, The model has to predict the probability distribution over vocabulary to get the maximum likely words in place of masked words. Thus, the task is called Masked Language modelling. . This approach of BERT achieved SotA in so many tasks. . This can be seen as a denoising autoencoder task. In which, the model is given an input with noise, and it has to output a denoised/cleaned representation of input(here, input:masked sentences. output: predictions for masks. thus, recovered output). . Generative Training . You might have heard of GAN(Generative Adversarial Networks). There is a generator and a discriminator. You can think of this as a game of &quot;chor(thief)-police&quot;. . Generator: Generate samples that are so realistic from a distribution of input samples and the goal here is to fool the discriminator. (chor) . Discriminator: Criticize the output generated by the generator about whether it is a real or a fake sample. (police) . The training is interesting! The better either one gets, other also improves! . Electrifying the training . We refreshed the ideas of MLM and GANs, but how they relate to ELECTRA. Turns out very much. ELECTRA introduced a new learning methodology called &quot;replaced token detection&quot;. You get a rough idea of what it could be. The input is again corrupted but instead of masking, the tokens are replaced by other tokens. and the task is to detect whether a token(a word represented by a number) is genuine or corrupted(real or fake). A new self-supervised task for language representation learning. . Now comes the GAN part. MLM trains model as a generator, while replaced token detection trains as a discriminator. The advantage is that the model gets to learn from all tokens, not just the masked out small sub-set. However, one thing to note here is that the training is not adversarial. The model is trained with maximum likelihood rather than adversarial loss due to the difficulty of applying GAN to text. . So, what does ELECTRA stands for? &quot;Efficiently Learning an Encoder that Classifies Token Replacements Accurately&quot; . . As we can observe from the training progress, The ELECTRA seems to converge faster. The reason is that the model is learning from all the tokens, thus it gets more ideas of tokens and the language compared to MLM tasks. This seems more robust. Look at the image on right, ELECTRA matched similar performances of RoBERTa and XLNet with about 25% of the compute! (FAST) . This is about the discriminator, but what about the generator(how to get the corrupted tokens). Should we do it manually? The GAN approach is at the rescue. We can use a generator model(preferably small) and ask it to predict a distribution(probability) over tokens and then the new tokens can be fed to the discriminator. . From where do the replacements come? You guessed right. A generator to adultrate the inputs. A small model that can give distribution over tokens can be used. Authors have used a BERT-small as a generator on a Masked Language Modelling task. (reminder, it is trained on maximum likelihood, instead of a adverserial loss.) Another observation, The Generator isn&#39;t given a noise vector as input like GANs are given. . . Around 15% of the tokens are masked. The outputs of Generator is fed to Discriminator. Thus we have can diffrentiate between a real(not masked) and a corrupted(masked and predicted by Generator) tokens on which the Discriminator can be trained(on a loss function). . So, is the discriminator always distinguishes on 15% of the tokens that are corrupted? No. . Isn&#39;t the goal of MLM is to generate correct words? and won&#39;t this lead to only real tokens for descriminator? Yes. If the generator outputs correct word then it is considered &quot;real&quot; when fed to the discriminator. Authors found this formulation to perform better on downstream tasks. . LOSS(ES) . Generator loss: . . let&#39;s take an example, . vocabulary: [During, this, pandemic, everyone, should, wear, a, mask, and, take, care, of, themselves] . inputG: During this pandemic, Everyone should [mask] a mask and take [mask] of themselves. . outputG: . [MASK]1: [0.06253422, 0.10410041, 0.0609353 , 0.0611319 , 0.07645429, 0.0834228 , 0.05501198, 0.09041031, 0.06122133, 0.09231945, 0.05474751, 0.0673528 , 0.1303577 ], a probability distribution over every token in vocabulary. here, the selected token would be &quot;themselves&quot; . for [mask]1: where the answer should be &quot;wear&quot;, so probability of &quot;wear&quot; should be high but the output is &quot;themselves&quot;.so the sentence that will be fed to Discriminator will be. . corrupted sentence: During this pandemic, Everyone should themselves a mask and take care of themselves. . Discriminator Loss . . inputD: During this pandemic, Everyone should themselves a mask and take care of themselves. . outputD:[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] . Combined loss . . NOTE: Backpropagation updates from this loss isn&#39;t done on generator. . This has resulted in a very good performance in so many benchmarks. It achieves State-of-the-art results in SQuAD 2.0. . The released code is very simple to train on GPU and you can explore more using Colab. Three pre-trained models ELECTRA-small, ELECTRA-base, and ELECTRA-Large are also released. . So, if you want to train or fine-tune your own models, checkout https://github.com/google-research/electra . I will be sharing my colab notebook with this. Hope this effort to explain helped you. . Stay tuned. Take care and stay safe. . . Acknowledgments: Thanks to Manu Romero for suggeting ELECTRA for my model . Currently, Working on Language models for Indian Languages (Gujarati, Samskrit, and Hindi as of now). I will upload the code soon. Looking forward for suggestions and feedback. . References . Google Blog on ELECTRA . | ELECTRA Paper . | BERT . | Attention is all you need . | .",
            "url": "https://parmarsuraj99.github.io/suraj-parmar/jupyter/nlp/2020/04/14/ELECTRA.html",
            "relUrl": "/jupyter/nlp/2020/04/14/ELECTRA.html",
            "date": " ‚Ä¢ Apr 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello, I am Suraj Parmar. I love to explain things I know. This help me to improve and test my understanding. I am setting up this blog in fastpages. I have written previously on medium here. . Stay tuned and stay safe. . This website is powered by fastpages. .",
          "url": "https://parmarsuraj99.github.io/suraj-parmar/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://parmarsuraj99.github.io/suraj-parmar/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}